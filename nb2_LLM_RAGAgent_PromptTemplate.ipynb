{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce7ad07-db0c-4ddf-84de-38e7bf2ec3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## comment\n",
    "## Agent + LLM + RAG + PromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee25ff3-7e4c-4fb6-9567-0b8e22f48ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install\n",
    "# !pip install langchain deeplake openai transformers sentencepiece sentence-transformers python-dotenv\n",
    "# !pip install --upgrade openai\n",
    "# /bin/bash: pip cache purge\n",
    "# /bin/bash: conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# /bin/bash: pip uninstall -y langchain openai\n",
    "# /bin/bash: pip install langchain==0.0.316 openai==0.28.1\n",
    "# /bin/bash: pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c896c917-380e-47bf-85c3-4995d271acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain import PromptTemplate\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from typing import List\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI  # Use ChatOpenAI for chat-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466548ef-c063-41ed-b211-b7c73b11fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class, function, global-variable\n",
    "\n",
    "## Custom Retriever that limits the number of documents returned\n",
    "class CustomRetriever:\n",
    "    def __init__(self, retriever, limit=1):\n",
    "        self.retriever = retriever\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        try:\n",
    "            # Retrieve documents and limit the number returned\n",
    "            docs = self.retriever.get_relevant_documents(query)\n",
    "            return docs[:self.limit]\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in CustomRetriever: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        try:\n",
    "            # Asynchronous retrieval (if needed)\n",
    "            docs = await self.retriever.aget_relevant_documents(query)\n",
    "            return docs[:self.limit]\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in CustomRetriever: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "## Open-source embeddings using SentenceTransformer\n",
    "class CustomEmbeddings:\n",
    "    def __init__(self):\n",
    "        super(CustomEmbeddings, self).__init__()\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        return self.embedding_model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embedding_model.encode(text, convert_to_tensor=False).tolist()\n",
    "    \n",
    "## return context based on question, file resource, and limit\n",
    "def get_context(question, filename, limit):\n",
    "    ## documents\n",
    "    loader = TextLoader(filename, encoding='utf-8')\n",
    "    docs_from_file = loader.load()\n",
    "    ## chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "    docs = text_splitter.split_documents(docs_from_file)\n",
    "    ## embeddings\n",
    "    embeddings = CustomEmbeddings()\n",
    "    dataset_path = './my_deeplake_dataset'\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings, overwrite=True)\n",
    "    db.add_documents(docs)\n",
    "    ## retriever\n",
    "    retriever = db.as_retriever()\n",
    "    custom_retriever = CustomRetriever(retriever, limit=limit)\n",
    "    ## context\n",
    "    retrieved_docs = custom_retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2a3438-2575-40a2-a2d3-be647bd68958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchowdh5/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c047a9f1d4fd44d5adbf13a99d74bb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "## LLM, tokenizer, pipeline\n",
    "try:\n",
    "    print(model)\n",
    "except Exception as e:\n",
    "    model_name = \"tiiuae/falcon-7b\"\n",
    "    cache_dir = \"/scratch/tmp/\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=1, max_length=512)  \n",
    "# llm = HuggingFacePipeline(pipeline=llm_pipeline) ## open source\n",
    "llm=ChatOpenAI(model_name='gpt-3.5-turbo') ## proprietary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a92898c-0b7b-4664-ae93-4bc7aafcd944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 545, which is longer than the specified 200\n",
      "Created a chunk of size 548, which is longer than the specified 200\n",
      "/home/mchowdh5/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n",
      "Creating 4 embeddings in 1 batches of size 4:: 100%|█████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./my_deeplake_dataset', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (4, 1)     str     None   \n",
      " metadata     json      (4, 1)     str     None   \n",
      " embedding  embedding  (4, 384)  float32   None   \n",
      "    id        text      (4, 1)     str     None   \n",
      "#########################################################################\n",
      "Meruem\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "## run: LLM + RAG + PromptTemplate\n",
    "\n",
    "question = 'who is the ultimate boss?'\n",
    "context = get_context(question, 'my_file.txt', 1) ## RAG\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template) ## prompt-engineering\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt) ## llm\n",
    "response = llm_chain.run({\"context\": context, \"question\": question})\n",
    "print('#########################################################################')\n",
    "print(response)\n",
    "print('#########################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b535d2c5-3887-4ef9-b4e4-ede00e2428ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "          (query_key_value): FalconLinear(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): FalconLinear(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): FalconLinear(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n",
      "#########################################################################\n",
      "Embedding(65024, 4544)\n",
      "FalconDecoderLayer(\n",
      "  (self_attention): FalconAttention(\n",
      "    (rotary_emb): FalconRotaryEmbedding()\n",
      "    (query_key_value): FalconLinear(in_features=4544, out_features=4672, bias=False)\n",
      "    (dense): FalconLinear(in_features=4544, out_features=4544, bias=False)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): FalconMLP(\n",
      "    (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176, bias=False)\n",
      "    (act): GELUActivation()\n",
      "    (dense_4h_to_h): FalconLinear(in_features=18176, out_features=4544, bias=False)\n",
      "  )\n",
      "  (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "Linear(in_features=4544, out_features=65024, bias=False)\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "print('#########################################################################')\n",
    "print(model)\n",
    "print('#########################################################################')\n",
    "print(model.transformer.word_embeddings)\n",
    "print(model.transformer.h[0])\n",
    "print(model.transformer.ln_f)\n",
    "print(model.lm_head)\n",
    "print('#########################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9e1369-58f4-4d80-8fe3-69d398afca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 545, which is longer than the specified 200\n",
      "Created a chunk of size 548, which is longer than the specified 200\n",
      "/home/mchowdh5/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n",
      "Creating 4 embeddings in 1 batches of size 4:: 100%|█████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./my_deeplake_dataset', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (4, 1)     str     None   \n",
      " metadata     json      (4, 1)     str     None   \n",
      " embedding  embedding  (4, 384)  float32   None   \n",
      "    id        text      (4, 1)     str     None   \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use tool_1 to generate a response based on the context and question.\n",
      "Action: tool_1\n",
      "Action Input: meruem is the ultimate boss.\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but your question is unclear. Can you please provide more information or clarify what you are asking?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to provide a more specific question about who the ultimate boss is.\n",
      "Action: tool_1\n",
      "Action Input: who is the ultimate boss?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but it seems like the question is incomplete. Can you please provide more information or clarify the question so I can help you better?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should rephrase the question to make it clearer.\n",
      "Action: tool_1\n",
      "Action Input: Who is considered the ultimate boss in the anime \"Hunter x Hunter\"?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but it seems like the question is missing. Could you please provide more information or clarify the question?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should specify that I am looking for a specific character.\n",
      "Action: tool_1\n",
      "Action Input: Who is the ultimate boss in \"Hunter x Hunter\"?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but the question \"h\" does not provide enough information for me to give a concise answer. Could you please provide more context or clarify the question?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should provide more context about the anime to get a specific answer.\n",
      "Action: tool_1\n",
      "Action Input: Who is the ultimate boss in the anime \"Hunter x Hunter\" known for his incredible strength and intelligence?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but it seems like the question is missing. Can you please provide more information or clarify your question?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should try to provide a simpler and more direct question.\n",
      "Action: tool_1\n",
      "Action Input: Who is the ultimate boss in \"Hunter x Hunter\"?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, I cannot provide an answer without a question.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The ultimate boss in \"Hunter x Hunter\" is Meruem.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "#########################################################################\n",
      "The ultimate boss in \"Hunter x Hunter\" is Meruem.\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "## run: Agent + LLM + RAG + PromptTemplate\n",
    "\n",
    "question = 'who is the ultimate boss?'\n",
    "context = get_context(question, 'my_file.txt', 1) ## RAG\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question very concisely.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)  # LLM setup\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='tool_1',\n",
    "        description='Use this tool to generate a response given the context and question.',\n",
    "        func=lambda input: llm_chain.run({\n",
    "            'context': input[0],\n",
    "            'question': input[1]\n",
    "        }),\n",
    "    ),\n",
    "]\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "response = agent.run(input=[context, question])\n",
    "print('#########################################################################')\n",
    "print(response)\n",
    "print('#########################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331ab15-cd09-41bc-928f-4fe819c234d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6ec50-cfe7-4ccc-8eee-296b8c0f0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8edfbc-9e65-4615-9b87-09d4f2eb48fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
