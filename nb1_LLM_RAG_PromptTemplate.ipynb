{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce7ad07-db0c-4ddf-84de-38e7bf2ec3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## comment\n",
    "## LLM + RAG + PromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee25ff3-7e4c-4fb6-9567-0b8e22f48ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install\n",
    "# !pip install langchain deeplake openai transformers sentencepiece sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c896c917-380e-47bf-85c3-4995d271acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain import PromptTemplate\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466548ef-c063-41ed-b211-b7c73b11fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class, function\n",
    "\n",
    "## Custom Retriever that limits the number of documents returned\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, retriever, limit=1):\n",
    "        self.retriever = retriever\n",
    "        self.limit = limit\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Retrieve documents and limit the number returned\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        return docs[:self.limit]\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Asynchronous retrieval (if needed)\n",
    "        docs = await self.retriever.aget_relevant_documents(query)\n",
    "        return docs[:self.limit]\n",
    "    \n",
    "## Open-source embeddings using SentenceTransformer\n",
    "class CustomEmbeddings:\n",
    "    def __init__(self):\n",
    "        super(CustomEmbeddings, self).__init__()\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        return self.embedding_model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embedding_model.encode(text, convert_to_tensor=False).tolist()\n",
    "    \n",
    "## return context based on question, file resource, and limit\n",
    "def get_context(question, filename, limit):\n",
    "    ## documents\n",
    "    loader = TextLoader(filename, encoding='utf-8')\n",
    "    docs_from_file = loader.load()\n",
    "    ## chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "    docs = text_splitter.split_documents(docs_from_file)\n",
    "    ## embeddings\n",
    "    embeddings = CustomEmbeddings()\n",
    "    dataset_path = './my_deeplake_dataset'\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings, overwrite=True)\n",
    "    db.add_documents(docs)\n",
    "    ## retriever\n",
    "    retriever = db.as_retriever()\n",
    "    custom_retriever = CustomRetriever(retriever, limit=limit)\n",
    "    ## context\n",
    "    retrieved_docs = custom_retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2a3438-2575-40a2-a2d3-be647bd68958",
   "metadata": {},
   "outputs": [],
   "source": [
    "## opensourcedLLM model, tokenizer, pipeline\n",
    "# model_name = \"tiiuae/falcon-7b\"\n",
    "# cache_dir = \"/scratch/tmp/\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "# llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=2, max_length=512)  \n",
    "# llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f41c459-d53d-4c67-8430-438c4a2b7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 302, which is longer than the specified 200\n",
      "Created a chunk of size 545, which is longer than the specified 200\n",
      "Created a chunk of size 548, which is longer than the specified 200\n",
      "/home/mchowdh5/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Creating 4 embeddings in 1 batches of size 4:: 100%|█████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.53it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./my_deeplake_dataset', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      "   text       text      (4, 1)     str     None   \n",
      " metadata     json      (4, 1)     str     None   \n",
      " embedding  embedding  (4, 384)  float32   None   \n",
      "    id        text      (4, 1)     str     None   \n",
      "#########################################################################\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "\n",
      "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
      "\n",
      "Context: meruem is the ultimate boss.\n",
      "\n",
      "Question: who is the ultimate boss?\n",
      "\n",
      "Answer:\n",
      "\n",
      "meruem is the ultimate boss.\n",
      "#########################################################################\n"
     ]
    }
   ],
   "source": [
    "## run: openSourcedLLM + RAG + PromptTemplate\n",
    "\n",
    "question = 'who is the ultimate boss?'\n",
    "context = get_context(question, 'my_file.txt', 1) ## RAG\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question very very concisely.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template) ## prompt-engineering\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt) ## llm\n",
    "response = llm_chain.run({\"context\": context, \"question\": question})\n",
    "print('#########################################################################')\n",
    "print(response)\n",
    "print('#########################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6b8f9b9-57ee-4d88-bc5b-056a8d7bd2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(65024, 4544)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (rotary_emb): FalconRotaryEmbedding()\n",
      "          (query_key_value): FalconLinear(in_features=4544, out_features=4672, bias=False)\n",
      "          (dense): FalconLinear(in_features=4544, out_features=4544, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (dense_4h_to_h): FalconLinear(in_features=18176, out_features=4544, bias=False)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n",
      "#########################################################################\n",
      "Embedding(65024, 4544)\n",
      "FalconDecoderLayer(\n",
      "  (self_attention): FalconAttention(\n",
      "    (rotary_emb): FalconRotaryEmbedding()\n",
      "    (query_key_value): FalconLinear(in_features=4544, out_features=4672, bias=False)\n",
      "    (dense): FalconLinear(in_features=4544, out_features=4544, bias=False)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): FalconMLP(\n",
      "    (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176, bias=False)\n",
      "    (act): GELUActivation()\n",
      "    (dense_4h_to_h): FalconLinear(in_features=18176, out_features=4544, bias=False)\n",
      "  )\n",
      "  (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
      "Linear(in_features=4544, out_features=65024, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print('#########################################################################')\n",
    "print(model.transformer.word_embeddings)\n",
    "print(model.transformer.h[0])\n",
    "print(model.transformer.ln_f)\n",
    "print(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331ab15-cd09-41bc-928f-4fe819c234d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
